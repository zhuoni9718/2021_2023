training roberta-large
dataset:<class 'dataloader.MultipleChoiceDataset'>
training epoch0
1.3551307335495948
0.7421488980017602
0.52440830372109
Epoch 1/8, Train Loss: 0.4369
Epoch: 1, Validation Accuracy: 0.9838536060279871,valid loss: 0.06405398905904242
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_0
training epoch1
0.03669505071686217
0.047730215042106464
0.05455069843105321
Epoch 2/8, Train Loss: 0.0509
Epoch: 2, Validation Accuracy: 0.9827771797631862,valid loss: 0.06620748635168916
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_1
training epoch2
0.022669209523236873
0.022404089886605902
0.021062637994440137
Epoch 3/8, Train Loss: 0.0205
Epoch: 3, Validation Accuracy: 0.9881593110871906,valid loss: 0.05406647683155444
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_2
training epoch3
0.0054896687059993576
0.006327261167359801
0.00918704396944548
Epoch 4/8, Train Loss: 0.0103
Epoch: 4, Validation Accuracy: 0.9838536060279871,valid loss: 0.04595144040892232
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_3
training epoch4
0.00902626026740577
0.008581200638488279
0.007528726797141201
Epoch 5/8, Train Loss: 0.0077
Epoch: 5, Validation Accuracy: 0.984930032292788,valid loss: 0.0732233482079531
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_4
training epoch5
0.006446973326773433
0.007085608740236564
0.007315100863827651
Epoch 6/8, Train Loss: 0.0074
Epoch: 6, Validation Accuracy: 0.984930032292788,valid loss: 0.06942936730181569
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_5
training epoch6
0.0015605682122083798
0.0035332891210717032
0.006050150653649285
Epoch 7/8, Train Loss: 0.0054
Epoch: 7, Validation Accuracy: 0.9860064585575888,valid loss: 0.048767459218357186
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_6
training epoch7
0.002225393660969881
0.005324352251285567
0.004561348075745532
Epoch 8/8, Train Loss: 0.0045
Epoch: 8, Validation Accuracy: 0.9860064585575888,valid loss: 0.053990959009529306
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_7
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/roberta-large_4
test Accuracy: 0.5249795249795249
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 388/388 [08:57<00:00,  1.38s/it]
100% 59/59 [00:27<00:00,  2.12it/s]
100% 388/388 [08:57<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [08:57<00:00,  1.38s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [08:55<00:00,  1.38s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [08:55<00:00,  1.38s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [08:56<00:00,  1.38s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [08:55<00:00,  1.38s/it]
100% 59/59 [00:27<00:00,  2.14it/s]
100% 388/388 [08:55<00:00,  1.38s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 77/77 [00:32<00:00,  2.37it/s]