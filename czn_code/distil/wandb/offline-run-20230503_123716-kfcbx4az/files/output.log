training roberta-large
dataset:<class 'dataloader.MultipleChoiceDataset'>
training epoch0
1.6150416779518126
1.6128570634126662
1.5241017897923788
Epoch 1/8, Train Loss: 1.4268
Epoch: 1, Validation Accuracy: 0.6592956592956593,valid loss: 0.9230926323246647
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_0
training epoch1
0.8554242572188377
0.8382363423705101
0.8035557870566845
Epoch 2/8, Train Loss: 0.7852
Epoch: 2, Validation Accuracy: 0.714987714987715,valid loss: 0.7569690257697911
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_1
training epoch2
0.431376762650907
0.4543713898025453
0.4524947545925776
Epoch 3/8, Train Loss: 0.4539
Epoch: 3, Validation Accuracy: 0.737919737919738,valid loss: 0.7523826534871931
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_2
training epoch3
0.250501219406724
0.2417738010548055
0.24438586370398602
Epoch 4/8, Train Loss: 0.2465
Epoch: 4, Validation Accuracy: 0.7403767403767404,valid loss: 1.0049709115516057
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_3
training epoch4
0.1333736860821955
0.1390501715213759
0.14701255062400984
Epoch 5/8, Train Loss: 0.1502
Epoch: 5, Validation Accuracy: 0.7297297297297297,valid loss: 1.185337553695812
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_4
training epoch5
0.09947052509058267
0.10135205344471615
0.0985541575762909
Epoch 6/8, Train Loss: 0.0971
Epoch: 6, Validation Accuracy: 0.7248157248157249,valid loss: 1.434112542094179
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_5
training epoch6
0.05744931847788393
0.06719145683688112
0.06924318226466615
Epoch 7/8, Train Loss: 0.0670
Epoch: 7, Validation Accuracy: 0.7387387387387387,valid loss: 1.4326435542520282
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_6
training epoch7
0.051480547293831475
0.05204373550510354
0.04820388391621236
Epoch 8/8, Train Loss: 0.0502
Epoch: 8, Validation Accuracy: 0.742014742014742,valid loss: 1.5137899953308032
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_7
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_1
test Accuracy: 0.714987714987715
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 388/388 [08:31<00:00,  1.32s/it]
100% 77/77 [00:33<00:00,  2.27it/s]
100% 388/388 [08:34<00:00,  1.33s/it]
100% 77/77 [00:33<00:00,  2.27it/s]
100% 388/388 [08:29<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.30it/s]
100% 388/388 [08:30<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.29it/s]
100% 388/388 [08:30<00:00,  1.32s/it]
100% 77/77 [00:33<00:00,  2.29it/s]
100% 388/388 [08:29<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.29it/s]
100% 388/388 [08:30<00:00,  1.32s/it]
100% 77/77 [00:33<00:00,  2.29it/s]
100% 388/388 [08:30<00:00,  1.32s/it]
100% 77/77 [00:33<00:00,  2.29it/s]
100% 77/77 [00:33<00:00,  2.31it/s]