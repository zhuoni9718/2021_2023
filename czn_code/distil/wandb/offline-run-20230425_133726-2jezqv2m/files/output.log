training roberta-large
training epoch0
Epoch 1/10, Train Loss: 1.6166
Epoch: 1, Validation Accuracy: 0.21959095801937567,valid loss: 1.6092615142912312
training epoch1
Epoch 2/10, Train Loss: 1.6132
Epoch: 2, Validation Accuracy: 0.21959095801937567,valid loss: 1.6092615142912312
training epoch2
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1549/1549 [10:42<00:00,  2.41it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 233/233 [00:35<00:00,  6.58it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1549/1549 [10:58<00:00,  2.35it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 233/233 [00:32<00:00,  7.15it/s]
  0%|▎                                                                                                                                        | 4/1549 [00:01<12:03,  2.14it/s]
Traceback (most recent call last):
  File "/users5/znchen/distil/predict_frame.py", line 147, in <module>
    main()
  File "/users5/znchen/distil/predict_frame.py", line 138, in main
    train(args.model_name,args.epochs,args.train_data,args.dev_data)
  File "/users5/znchen/distil/predict_frame.py", line 58, in train
    optimizer.step()
  File "//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py", line 447, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
KeyboardInterrupt