training roberta-large
dataset:<class 'dataloader.MultipleChoiceDataset'>
training epoch0
1.4249340066313743
0.8041443818469998
0.571000012911633
Epoch 1/8, Train Loss: 0.4755
Epoch: 1, Validation Accuracy: 0.9795479009687836,valid loss: 0.07641716164258561
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_0
training epoch1
0.07644972903099187
0.0965552806568212
0.10015925715688657
Epoch 2/8, Train Loss: 0.0928
Epoch: 2, Validation Accuracy: 0.9795479009687836,valid loss: 0.08576876644187867
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_1
training epoch2
0.04652847604723093
0.04771798634741394
0.051114334404699556
Epoch 3/8, Train Loss: 0.0529
Epoch: 3, Validation Accuracy: 0.9773950484391819,valid loss: 0.12147548634712906
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_2
training epoch3
0.01993733510795991
0.01909972222456971
0.023536435571631476
Epoch 4/8, Train Loss: 0.0245
Epoch: 4, Validation Accuracy: 0.9870828848223897,valid loss: 0.08098168170392044
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_3
training epoch4
0.01739078451328274
0.017021700800524394
0.015338889066309017
Epoch 5/8, Train Loss: 0.0169
Epoch: 5, Validation Accuracy: 0.9881593110871906,valid loss: 0.060356910894899155
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_4
training epoch5
0.008258999215024336
0.0069746110775735625
0.006562093032480923
Epoch 6/8, Train Loss: 0.0058
Epoch: 6, Validation Accuracy: 0.9892357373519914,valid loss: 0.0671483591101105
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_5
training epoch6
0.005199312909709443
0.005719105298645189
0.008756694547295904
Epoch 7/8, Train Loss: 0.0087
Epoch: 7, Validation Accuracy: 0.9892357373519914,valid loss: 0.056319221402854565
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_6
training epoch7
0.003725516316943116
0.008673771453192132
0.006721101539009182
Epoch 8/8, Train Loss: 0.0061
Epoch: 8, Validation Accuracy: 0.9903121636167922,valid loss: 0.05320889787885016
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_7
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/roberta-large_4
test Accuracy: 0.47911547911547914
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 388/388 [09:00<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [09:02<00:00,  1.40s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [08:59<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.14it/s]
100% 388/388 [08:58<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.14it/s]
100% 388/388 [08:59<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.13it/s]
100% 388/388 [08:58<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.14it/s]
100% 388/388 [08:58<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.14it/s]
100% 388/388 [08:58<00:00,  1.39s/it]
100% 59/59 [00:27<00:00,  2.14it/s]
100% 77/77 [00:32<00:00,  2.36it/s]