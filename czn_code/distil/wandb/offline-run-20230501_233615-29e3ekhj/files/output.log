training roberta-large
dataset:<class 'dataloader.MultipleChoiceDataset'>
training epoch0
1.6177556490898133
1.6172171038389207
1.567362067302068
1.482374475002289
1.4041618775129319
Epoch 1/8, Train Loss: 1.3778
Epoch: 1, Validation Accuracy: 0.6994266994266994,valid loss: 0.7733247129174022
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_0
training epoch1
0.8779201796650886
0.8511242592334747
0.8674904969334603
0.852049855068326
0.8399245957732201
Epoch 2/8, Train Loss: 0.8346
Epoch: 2, Validation Accuracy: 0.7526617526617526,valid loss: 0.6700933171170098
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_1
training epoch2
0.5422332221269608
0.5367973370105028
0.5405743513007959
0.5388915580138565
0.5374190369546413
Epoch 3/8, Train Loss: 0.5375
Epoch: 3, Validation Accuracy: 0.76003276003276,valid loss: 0.6766784969088319
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_2
training epoch3
0.31250324871391055
0.3112265478633344
0.32279035447786253
0.3275877879373729
0.3290543407201767
Epoch 4/8, Train Loss: 0.3303
Epoch: 4, Validation Accuracy: 0.7534807534807535,valid loss: 0.7500659207051451
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_3
training epoch4
0.22125035984441638
0.2196415440645069
0.21578652736730874
0.2197418466047384
0.21597887840494515
Epoch 5/8, Train Loss: 0.2180
Epoch: 5, Validation Accuracy: 0.7477477477477478,valid loss: 0.9511401467509084
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_4
training epoch5
0.17048901440110056
0.1508019795198925
0.15062871246394935
0.14754941176935973
0.14367620160774094
Epoch 6/8, Train Loss: 0.1427
Epoch: 6, Validation Accuracy: 0.733005733005733,valid loss: 1.1660260199145838
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_5
training epoch6
0.1104719457635656
0.11477536264515947
0.10907970649306663
0.09978763828316005
0.10591430500964634
Epoch 7/8, Train Loss: 0.1046
Epoch: 7, Validation Accuracy: 0.7485667485667485,valid loss: 1.3168511296828072
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_6
training epoch7
0.07406031849182909
0.07547579140242305
0.07649822739064499
0.07309705837611545
0.07539163200426265
Epoch 8/8, Train Loss: 0.0755
Epoch: 8, Validation Accuracy: 0.7411957411957412,valid loss: 1.3848831440721239
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_7
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_1
test Accuracy: 0.7526617526617526
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 533/533 [11:36<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:30<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.33it/s]
100% 533/533 [11:29<00:00,  1.29s/it]
100% 77/77 [00:32<00:00,  2.33it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:31<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:30<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 77/77 [00:33<00:00,  2.33it/s]