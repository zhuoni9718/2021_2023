training roberta-large
dataset:<class 'dataloader.MultipleChoiceDataset'>
training epoch0
1.6168130362033843
1.6174037140607833
1.60650772412618
1.5693737694621086
1.508480867624283
Epoch 1/30, Train Loss: 1.4865
Epoch: 1, Validation Accuracy: 0.6748566748566749,valid loss: 0.8899240408624921
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_0
training epoch1
1.0443150228261948
1.0025838938355447
0.989112130800883
0.9672416800260544
0.950550641298294
Epoch 2/30, Train Loss: 0.9417
Epoch: 2, Validation Accuracy: 0.7403767403767404,valid loss: 0.6711778416262044
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_1
training epoch2
0.6935499192774296
0.6818008849769831
0.6920553378760814
0.6888204958662391
0.6877867026627064
Epoch 3/30, Train Loss: 0.6854
Epoch: 3, Validation Accuracy: 0.76003276003276,valid loss: 0.6690376708646874
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_2
training epoch3
0.5175235162675381
0.51009930126369
0.5096383300920327
0.5182620077952742
0.518071207806468
Epoch 4/30, Train Loss: 0.5198
Epoch: 4, Validation Accuracy: 0.7657657657657657,valid loss: 0.6483784954656254
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_3
training epoch4
0.384996742233634
0.38440237179398534
0.39002998650074006
0.3912402364052832
0.38794758063554763
Epoch 5/30, Train Loss: 0.3905
Epoch: 5, Validation Accuracy: 0.7567567567567568,valid loss: 0.7276986908990067
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_4
training epoch5
0.31505376171320676
0.2901830154005438
0.2945798271956543
0.2868351148208603
0.29173279666528107
Epoch 6/30, Train Loss: 0.2943
Epoch: 6, Validation Accuracy: 0.7518427518427518,valid loss: 0.8391757215578834
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_5
training epoch6
0.23293593246489763
0.23365153204649686
0.23369260923316082
0.22534655921161176
0.2302807585950941
Epoch 7/30, Train Loss: 0.2289
Epoch: 7, Validation Accuracy: 0.7542997542997543,valid loss: 1.1375009720395137
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_6
training epoch7
0.15804249856621028
0.15579041032353416
0.16551987127944207
0.16916976838663686
0.17795374699821695
Epoch 8/30, Train Loss: 0.1783
Epoch: 8, Validation Accuracy: 0.7452907452907452,valid loss: 1.1401495742913965
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_7
training epoch8
0.13987695729010738
0.14746436951158104
0.14750913347078798
0.14643526986561484
0.1489914338493254
Epoch 9/30, Train Loss: 0.1487
Epoch: 9, Validation Accuracy: 0.7469287469287469,valid loss: 1.2606776521190421
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_8
training epoch9
0.1422974757052725
0.11421609696786618
0.11790220688242697
0.11720882894551324
0.12265334118640749
Epoch 10/30, Train Loss: 0.1229
Epoch: 10, Validation Accuracy: 0.7395577395577395,valid loss: 1.5279906999755215
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_9
training epoch10
0.08377189589140471
0.0906032236595638
0.1007168134227201
0.10083528566108725
0.10169011184287956
Epoch 11/30, Train Loss: 0.1019
Epoch: 11, Validation Accuracy: 0.7387387387387387,valid loss: 1.4334818774035998
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_10
training epoch11
0.07822486839722842
0.07680067985551432
0.08568807578393413
0.08962307188772684
0.09009747538246665
Epoch 12/30, Train Loss: 0.0871
Epoch: 12, Validation Accuracy: 0.733005733005733,valid loss: 1.661280131596443
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_11
training epoch12
0.06356168363949109
0.06239379575203202
0.060686369173769586
0.06250947771964092
0.07007483614973899
Epoch 13/30, Train Loss: 0.0691
Epoch: 13, Validation Accuracy: 0.737919737919738,valid loss: 1.6984974044532357
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_12
training epoch13
0.06726503085580589
0.06017494216356681
0.0569197884010949
0.05959594577911957
0.05969252566018622
Epoch 14/30, Train Loss: 0.0600
Epoch: 14, Validation Accuracy: 0.7297297297297297,valid loss: 1.8243569278871858
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_13
training epoch14
0.04901282880728104
0.05064443656186399
0.05311782546179738
0.05108762675157934
0.0573579477633715
Epoch 15/30, Train Loss: 0.0576
Epoch: 15, Validation Accuracy: 0.7452907452907452,valid loss: 1.8936112439138937
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_14
training epoch15
0.04013778163655388
0.04013299436431453
0.045897041287850396
0.04507180954198702
0.04666926714813872
Epoch 16/30, Train Loss: 0.0482
Epoch: 16, Validation Accuracy: 0.7411957411957412,valid loss: 1.851528633047234
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_15
training epoch16
0.038088369690822216
0.03794904463277135
0.041143045955797486
0.041911041972720824
0.04351680112120232
Epoch 17/30, Train Loss: 0.0428
Epoch: 17, Validation Accuracy: 0.7411957411957412,valid loss: 1.857710365302764
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_16
training epoch17
0.019289133731356288
0.03339422113585442
0.032702377526453955
0.03322789585419912
0.03825638497970942
Epoch 18/30, Train Loss: 0.0370
Epoch: 18, Validation Accuracy: 0.7256347256347256,valid loss: 2.0257811586965215
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_17
training epoch18
0.03299852170273539
0.02778443593872396
0.024379726626637344
0.024549445450363406
0.027744506323914037
Epoch 19/30, Train Loss: 0.0276
Epoch: 19, Validation Accuracy: 0.7371007371007371,valid loss: 1.9520331496348629
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_18
training epoch19
0.040002646815457864
0.033200715613330656
0.031085082068353056
0.031656310296110705
0.03314148359023744
Epoch 20/30, Train Loss: 0.0324
Epoch: 20, Validation Accuracy: 0.7477477477477478,valid loss: 1.9288304431119014
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_19
training epoch20
0.023133826318814953
0.02434596488732012
0.022004675681587857
0.022517759270066762
0.02342691834450949
Epoch 21/30, Train Loss: 0.0252
Epoch: 21, Validation Accuracy: 0.7387387387387387,valid loss: 1.9681334513928983
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_20
training epoch21
0.028212959003808463
0.0224722224691196
0.019776724489693434
0.02217447141221605
0.02213155737129995
Epoch 22/30, Train Loss: 0.0224
Epoch: 22, Validation Accuracy: 0.7338247338247338,valid loss: 2.2130049670344922
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_21
training epoch22
0.031292570134188506
0.02473995889925561
0.02082589396974716
0.019955473327033353
0.02165613155486315
Epoch 23/30, Train Loss: 0.0220
Epoch: 23, Validation Accuracy: 0.7354627354627354,valid loss: 2.1965465583784094
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_22
training epoch23
0.008551112268009007
0.01435261189876975
0.013342167929038879
0.011810707282697565
0.016511620871738422
Epoch 24/30, Train Loss: 0.0163
Epoch: 24, Validation Accuracy: 0.7387387387387387,valid loss: 2.184390720814563
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_23
training epoch24
0.017663191177181795
0.01648647289191585
0.017750453046002253
0.018151030366412354
0.016300162109995846
Epoch 25/30, Train Loss: 0.0169
Epoch: 25, Validation Accuracy: 0.7362817362817363,valid loss: 2.183177463889278
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_24
training epoch25
0.0235139584850711
0.015974305419399995
0.016662493006301345
0.017189771976994183
0.018475853782283353
Epoch 26/30, Train Loss: 0.0180
Epoch: 26, Validation Accuracy: 0.7313677313677314,valid loss: 2.263288632054532
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_25
training epoch26
0.009890589603595288
0.011735434746552188
0.013398395450453364
0.013843838905890315
0.014839131867111327
Epoch 27/30, Train Loss: 0.0143
Epoch: 27, Validation Accuracy: 0.7289107289107289,valid loss: 2.1753834977023327
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_26
training epoch27
0.008691620889746758
0.013033105715214717
0.016710681799047222
0.015714115358917626
0.014091279736318598
Epoch 28/30, Train Loss: 0.0149
Epoch: 28, Validation Accuracy: 0.7395577395577395,valid loss: 2.19059534773666
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_27
training epoch28
0.011331176502260405
0.010976539514650661
0.01069444340948209
0.011882136203104583
0.012324094151688314
Epoch 29/30, Train Loss: 0.0118
Epoch: 29, Validation Accuracy: 0.7387387387387387,valid loss: 2.1940396317339794
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_28
training epoch29
0.0076245062277013175
0.009032900693264807
0.014163690845940002
0.014304434029694217
0.014312896187555324
Epoch 30/30, Train Loss: 0.0139
Epoch: 30, Validation Accuracy: 0.7403767403767404,valid loss: 2.201185758223352
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_29
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_1
test Accuracy: 0.7403767403767404
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:34<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:31<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:34<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:34<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:31<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.33it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:31<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:30<00:00,  1.30s/it]
100% 77/77 [00:32<00:00,  2.34it/s]
100% 533/533 [11:34<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.33it/s]
100% 533/533 [11:32<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 77/77 [00:33<00:00,  2.32it/s]