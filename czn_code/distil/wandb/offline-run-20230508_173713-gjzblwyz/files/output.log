training roberta-large
dataset:<class 'dataloader.MultipleChoiceDatasetCgk'>
training epoch0
1.6174849259853363
1.6185197830200195
1.617944787343343
1.6053911793231963
1.546673126578331
Epoch 1/30, Train Loss: 1.5201
Epoch: 1, Validation Accuracy: 0.6076986076986077,valid loss: 1.0340300953233397
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_0
training epoch1
1.0795149409770965
1.0402199295163155
1.016219901939233
0.9970954665541649
0.9808052475452423
Epoch 2/30, Train Loss: 0.9764
Epoch: 2, Validation Accuracy: 0.7215397215397216,valid loss: 0.745258456700808
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_1
training epoch2
0.7378552234172822
0.7173725964128971
0.7167852388819059
0.7148785330355167
0.7148607611656189
Epoch 3/30, Train Loss: 0.7102
Epoch: 3, Validation Accuracy: 0.7305487305487306,valid loss: 0.7274125973125557
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_2
training epoch3
0.46391737796366217
0.4879831214621663
0.48687014835576214
0.49305832223966717
0.48925554664433
Epoch 4/30, Train Loss: 0.4897
Epoch: 4, Validation Accuracy: 0.7411957411957412,valid loss: 0.7413743075418782
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_3
training epoch4
0.320650569871068
0.3297627974115312
0.3280157375211517
0.3400916560180485
0.33768327625840905
Epoch 5/30, Train Loss: 0.3398
Epoch: 5, Validation Accuracy: 0.7411957411957412,valid loss: 0.7974944278210788
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_4
training epoch5
0.244758817628026
0.2380186479073018
0.23014412203337997
0.22783911957521924
0.23345374444220215
Epoch 6/30, Train Loss: 0.2350
Epoch: 6, Validation Accuracy: 0.7493857493857494,valid loss: 0.9405468167229132
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_5
training epoch6
0.13588352943770587
0.15059066283982248
0.15648374817954996
0.1529494562066975
0.1559358591674827
Epoch 7/30, Train Loss: 0.1559
Epoch: 7, Validation Accuracy: 0.7256347256347256,valid loss: 1.2195289999053076
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_6
training epoch7
0.10875696598552168
0.10418444331749925
0.10923433699266752
0.11219710500714428
0.11381900462816702
Epoch 8/30, Train Loss: 0.1144
Epoch: 8, Validation Accuracy: 0.733005733005733,valid loss: 1.411905609337347
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_7
training epoch8
0.08389536276488797
0.08159585208843055
0.08096780678853975
0.07892237721000128
0.08530093637909159
Epoch 9/30, Train Loss: 0.0862
Epoch: 9, Validation Accuracy: 0.7272727272727273,valid loss: 1.6458371111082952
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_8
training epoch9
0.07393427999340929
0.06781847307840508
0.06909363724332555
0.0694849537122991
0.06886528733446176
Epoch 10/30, Train Loss: 0.0688
Epoch: 10, Validation Accuracy: 0.723996723996724,valid loss: 1.920628758655353
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_9
training epoch10
0.04810850426187244
0.04787129270034711
0.04606555783053106
0.047546480223563774
0.04772000459942501
Epoch 11/30, Train Loss: 0.0478
Epoch: 11, Validation Accuracy: 0.7125307125307125,valid loss: 1.8958378525136352
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_10
training epoch11
0.04006024940210409
0.037995608351438934
0.04105565373577216
0.042381776936531425
0.040165878905785574
Epoch 12/30, Train Loss: 0.0405
Epoch: 12, Validation Accuracy: 0.723996723996724,valid loss: 2.0055770107678006
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_11
training epoch12
0.03830810457130156
0.03304922292547644
0.031695663239757955
0.03187167128583919
0.03123458950528513
Epoch 13/30, Train Loss: 0.0312
Epoch: 13, Validation Accuracy: 0.7256347256347256,valid loss: 2.0446020867917443
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_12
training epoch13
0.01171943522282163
0.01579720569412416
0.018301350817550126
0.02082647001669045
0.018964267500643017
Epoch 14/30, Train Loss: 0.0188
Epoch: 14, Validation Accuracy: 0.7272727272727273,valid loss: 2.239589649796873
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_13
training epoch14
0.019100736213490562
0.025543424985413593
0.02171543247333697
0.023079196090059356
0.02369114321360756
Epoch 15/30, Train Loss: 0.0229
Epoch: 15, Validation Accuracy: 0.7231777231777232,valid loss: 2.3698544694812265
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_14
training epoch15
0.013170841406742967
0.015277860636431573
0.01724778748183843
0.020739771881417724
0.01945609415566404
Epoch 16/30, Train Loss: 0.0202
Epoch: 16, Validation Accuracy: 0.7346437346437347,valid loss: 2.6793677028897522
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_15
training epoch16
0.015207826016804659
0.02290669548441043
0.01894242421478688
0.01918685221435587
0.02037663940657631
Epoch 17/30, Train Loss: 0.0197
Epoch: 17, Validation Accuracy: 0.7289107289107289,valid loss: 2.4531585052851343
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_16
training epoch17
0.018229748845261896
0.021548543351821933
0.016232363220910883
0.01660631895642087
0.015661730819880062
Epoch 18/30, Train Loss: 0.0153
Epoch: 18, Validation Accuracy: 0.7289107289107289,valid loss: 2.4247636079014123
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_17
training epoch18
0.008464554607823799
0.010580080750123741
0.009556557041351681
0.010212826151265411
0.010257628816104465
Epoch 19/30, Train Loss: 0.0104
Epoch: 19, Validation Accuracy: 0.7207207207207207,valid loss: 2.447996827882606
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_18
training epoch19
0.014551846866141532
0.009689512371206348
0.011005869146313903
0.011047095549440576
0.011740053791258729
Epoch 20/30, Train Loss: 0.0115
Epoch: 20, Validation Accuracy: 0.7346437346437347,valid loss: 2.400177137418227
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_19
training epoch20
0.009480206714918529
0.010095372040862291
0.008268910118690374
0.008990764894841533
0.010388517789106922
Epoch 21/30, Train Loss: 0.0102
Epoch: 21, Validation Accuracy: 0.7354627354627354,valid loss: 2.2607465122233736
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_20
training epoch21
0.006349246675443734
0.011104785963125696
0.008181564343566637
0.008467489458729157
0.007943967156160013
Epoch 22/30, Train Loss: 0.0078
Epoch: 22, Validation Accuracy: 0.7321867321867321,valid loss: 2.4346320510104107
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_21
training epoch22
0.015949776497573856
0.013444444790699207
0.011326326612422643
0.011846694379873074
0.011509428130474886
Epoch 23/30, Train Loss: 0.0109
Epoch: 23, Validation Accuracy: 0.7248157248157249,valid loss: 2.4142876907989574
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_22
training epoch23
0.006117195743152024
0.008737758482650477
0.00848899410519667
0.007460676467192086
0.007715933454514624
Epoch 24/30, Train Loss: 0.0074
Epoch: 24, Validation Accuracy: 0.7117117117117117,valid loss: 2.5956046801882904
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_23
training epoch24
0.007595784434212663
0.0058652602680794
0.008956278194620537
0.010317413538219918
0.009937619826830236
Epoch 25/30, Train Loss: 0.0104
Epoch: 25, Validation Accuracy: 0.7223587223587223,valid loss: 2.519127314208777
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_24
training epoch25
0.016425791631215988
0.00944431656089425
0.009853043195684637
0.008935211956211726
0.008645597212945992
Epoch 26/30, Train Loss: 0.0082
Epoch: 26, Validation Accuracy: 0.7248157248157249,valid loss: 2.5998922430656175
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_25
training epoch26
0.005912046968332376
0.008414439869347992
0.006209289055951895
0.004964981490560685
0.007075196504950572
Epoch 27/30, Train Loss: 0.0070
Epoch: 27, Validation Accuracy: 0.7272727272727273,valid loss: 2.6371851441535084
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_26
training epoch27
0.0027140763508385923
0.003766958312739388
0.004897898107009785
0.004179698197690077
0.006523891219827614
Epoch 28/30, Train Loss: 0.0062
Epoch: 28, Validation Accuracy: 0.7256347256347256,valid loss: 2.611462342081132
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_27
training epoch28
0.011881664623705634
0.006728190560558134
0.005115846369374587
0.004682415111155838
0.006923631376416105
Epoch 29/30, Train Loss: 0.0065
Epoch: 29, Validation Accuracy: 0.7272727272727273,valid loss: 2.6283298154155927
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_28
training epoch29
0.003258377453739305
0.0033373031240746687
0.0031149493267434354
0.0028997155390933036
0.003653360969631403
Epoch 30/30, Train Loss: 0.0036
Epoch: 30, Validation Accuracy: 0.7289107289107289,valid loss: 2.6454328871392585
save to ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_29
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/bartrationaletrainroberta-large_1
test Accuracy: 0.7231777231777232
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 533/533 [12:28<00:00,  1.40s/it]
100% 77/77 [00:36<00:00,  2.08it/s]
100% 533/533 [12:30<00:00,  1.41s/it]
100% 77/77 [00:36<00:00,  2.10it/s]
100% 533/533 [12:28<00:00,  1.40s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:25<00:00,  1.40s/it]
100% 77/77 [00:35<00:00,  2.20it/s]
100% 533/533 [12:23<00:00,  1.39s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:27<00:00,  1.40s/it]
100% 77/77 [00:34<00:00,  2.20it/s]
100% 533/533 [12:25<00:00,  1.40s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:25<00:00,  1.40s/it]
100% 77/77 [00:34<00:00,  2.20it/s]
100% 533/533 [12:25<00:00,  1.40s/it]
100% 77/77 [00:35<00:00,  2.20it/s]
100% 533/533 [12:27<00:00,  1.40s/it]
100% 77/77 [00:35<00:00,  2.20it/s]
100% 533/533 [12:20<00:00,  1.39s/it]
100% 77/77 [00:35<00:00,  2.18it/s]
100% 533/533 [12:24<00:00,  1.40s/it]
100% 77/77 [00:35<00:00,  2.16it/s]
100% 533/533 [12:19<00:00,  1.39s/it]
100% 77/77 [00:35<00:00,  2.17it/s]
100% 533/533 [12:17<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:16<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:14<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.17it/s]
100% 533/533 [12:16<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:15<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.15it/s]
100% 533/533 [12:15<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:15<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.18it/s]
100% 533/533 [12:38<00:00,  1.42s/it]
100% 77/77 [00:38<00:00,  1.98it/s]
100% 533/533 [12:22<00:00,  1.39s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:14<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:14<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.16it/s]
100% 533/533 [12:16<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.19it/s]
100% 533/533 [12:48<00:00,  1.44s/it]
100% 77/77 [00:39<00:00,  1.94it/s]
100% 533/533 [12:47<00:00,  1.44s/it]
100% 77/77 [00:39<00:00,  1.97it/s]
100% 533/533 [12:41<00:00,  1.43s/it]
100% 77/77 [00:35<00:00,  2.20it/s]
100% 533/533 [12:16<00:00,  1.38s/it]
100% 77/77 [00:35<00:00,  2.15it/s]
100% 533/533 [12:14<00:00,  1.38s/it]
100% 77/77 [00:34<00:00,  2.20it/s]
100% 77/77 [00:35<00:00,  2.19it/s]