training roberta-large
dataset:<class 'dataloader.MultipleChoiceDataset'>
training epoch0
1.6168906784057617
1.6173594886064528
1.5754453082879385
1.4984899695217608
1.4355467666387558
Epoch 1/8, Train Loss: 1.4196
Epoch: 1, Validation Accuracy: 0.7534983853606028,valid loss: 0.7434698767581228
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_0
training epoch1
1.1026009315252303
1.0728424656391145
1.053813596566518
1.027254349514842
0.9900026761293411
Epoch 2/8, Train Loss: 0.9814
Epoch: 2, Validation Accuracy: 0.7954790096878364,valid loss: 0.5846832624431384
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_1
training epoch2
0.6676524080336094
0.6603512793779374
0.6609060264627139
0.6632359990477562
0.6581461218297482
Epoch 3/8, Train Loss: 0.6552
Epoch: 3, Validation Accuracy: 0.7879440258342304,valid loss: 0.5568415312948873
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_2
training epoch3
0.45445408605039117
0.4470574387907982
0.44665349620083966
0.45006740659475325
0.44841646909713745
Epoch 4/8, Train Loss: 0.4492
Epoch: 4, Validation Accuracy: 0.8073196986006459,valid loss: 0.5273475413352756
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_3
training epoch4
0.31335714023560285
0.31348702201619744
0.31630115896463395
0.31772407121025026
0.31658424774743615
Epoch 5/8, Train Loss: 0.3176
Epoch: 5, Validation Accuracy: 0.806243272335845,valid loss: 0.593342668503144
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_4
training epoch5
0.22735102612059563
0.22639150648145004
0.22045843279920518
0.21815349544165655
0.21999583041109144
Epoch 6/8, Train Loss: 0.2191
Epoch: 6, Validation Accuracy: 0.8105489773950484,valid loss: 0.7370614690027373
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_5
training epoch6
0.16556903158081696
0.17168811054318212
0.16641149867345423
0.1551633846182085
0.15976438308355864
Epoch 7/8, Train Loss: 0.1578
Epoch: 7, Validation Accuracy: 0.8073196986006459,valid loss: 0.7745880346638703
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_6
training epoch7
0.11099755202420056
0.1205925237305928
0.12374294369791945
0.12482922757451888
0.12435851470567286
Epoch 8/8, Train Loss: 0.1267
Epoch: 8, Validation Accuracy: 0.8094725511302476,valid loss: 0.8049564391208844
save to ./tmp/predict/promptk_QCK_rationale/roberta-large_7
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/roberta-large_7
test Accuracy: 0.7305487305487306
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 533/533 [11:45<00:00,  1.32s/it]
100% 59/59 [00:25<00:00,  2.33it/s]
100% 533/533 [11:48<00:00,  1.33s/it]
100% 59/59 [00:25<00:00,  2.35it/s]
100% 533/533 [11:49<00:00,  1.33s/it]
100% 59/59 [00:25<00:00,  2.35it/s]
100% 533/533 [11:47<00:00,  1.33s/it]
100% 59/59 [00:25<00:00,  2.34it/s]
100% 533/533 [11:50<00:00,  1.33s/it]
100% 59/59 [00:25<00:00,  2.35it/s]
100% 533/533 [11:48<00:00,  1.33s/it]
100% 59/59 [00:25<00:00,  2.34it/s]
100% 533/533 [11:50<00:00,  1.33s/it]
100% 59/59 [00:25<00:00,  2.35it/s]
100% 533/533 [11:49<00:00,  1.33s/it]
100% 59/59 [00:25<00:00,  2.35it/s]
100% 77/77 [00:32<00:00,  2.34it/s]