training roberta-large
dataset:<class 'dataloader.MultipleChoiceDataset'>
training epoch0
1.6159455895423889
1.616829553246498
1.5752290320396423
1.479768873900175
1.3980795592069626
Epoch 1/8, Train Loss: 1.3706
Epoch: 1, Validation Accuracy: 0.6863226863226863,valid loss: 0.7903176023588552
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_0
training epoch1
0.8611915987730027
0.859419092386961
0.8643687106172244
0.8478727864474058
0.8317501392960548
Epoch 2/8, Train Loss: 0.8276
Epoch: 2, Validation Accuracy: 0.7502047502047502,valid loss: 0.6509539821705261
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_1
training epoch2
0.5342508059740066
0.522615017965436
0.5276956036190192
0.5307421312853694
0.528213601961732
Epoch 3/8, Train Loss: 0.5267
Epoch: 3, Validation Accuracy: 0.7551187551187551,valid loss: 0.6907245718039475
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_2
training epoch3
0.3121068548411131
0.31229771201033146
0.3169403606808434
0.3207690490526147
0.3221577210593969
Epoch 4/8, Train Loss: 0.3242
Epoch: 4, Validation Accuracy: 0.7485667485667485,valid loss: 0.7973885745197148
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_3
training epoch4
0.2013183922879398
0.20607820474077015
0.2107196433407565
0.2117156929918565
0.21275633424520493
Epoch 5/8, Train Loss: 0.2144
Epoch: 5, Validation Accuracy: 0.7403767403767404,valid loss: 0.9653682571339917
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_4
training epoch5
0.15223853825824335
0.1518252217536792
0.15199761249590665
0.1459937027381966
0.1435869772876613
Epoch 6/8, Train Loss: 0.1434
Epoch: 6, Validation Accuracy: 0.7346437346437347,valid loss: 1.153627625339991
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_5
training epoch6
0.10573659385263454
0.1145167923925328
0.1040696467536812
0.10041303287929622
0.10509284091484733
Epoch 7/8, Train Loss: 0.1044
Epoch: 7, Validation Accuracy: 0.7289107289107289,valid loss: 1.2841225465783825
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_6
training epoch7
0.0661247714678757
0.07923583594238152
0.07905965964271067
0.07796761495716055
0.08242147522349842
Epoch 8/8, Train Loss: 0.0821
Epoch: 8, Validation Accuracy: 0.7321867321867321,valid loss: 1.3826967003670605
save to ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_7
testing roberta-large
testing with ./tmp/predict/promptk_QCK_rationale/t5rationaletrainroberta-large_1
test Accuracy: 0.7502047502047502
Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
//users5/znchen/anaconda3/envs/q2k/lib/python3.10/site-packages/transformers-4.27.0.dev0-py3.10.egg/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
100% 533/533 [11:33<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.31it/s]
100% 533/533 [11:35<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:35<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.31it/s]
100% 533/533 [11:34<00:00,  1.30s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:35<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:35<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:35<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.32it/s]
100% 533/533 [11:35<00:00,  1.31s/it]
100% 77/77 [00:33<00:00,  2.31it/s]
100% 77/77 [00:33<00:00,  2.30it/s]